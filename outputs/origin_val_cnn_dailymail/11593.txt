google wants to rank internet pages based on the quality of the facts they contain
a new paper published by a group of software engineers suggests that the internet search giant may be preparing to change the algorithms it uses to scour the web
currently searches appear according to a complex combination of key words and links with other websites , but this fails to weed out inaccurate information
google may be about to change the way it delivers search results to users by ranking results by accuracy instead a google research team has developed a way of measuring the trustworthiness of the information contained on an internet page
if implemented it could mean that currently popular sources that regularly get facts wrong could fall foul of the new search technique
not one to be outdone by its rivals , google is reportedly working on a mobile payment service called android pay
the firm is expected to officially announce the service at its developer conference later this year
it will create a way for companies to accept transactions through their apps without having to introduce their own individual payment services
android pay users will then be able to upload credit card or debit card information to a single secure location but use it to pay for items across apps
and customers will be able to use it to pay for in - app purchases , goods or services with a single tap
google is also expected to allow companies to use its android pay api to enable tap - to - pay options in physical stores using nfc readers , for example
in a paper to be published in the proceedings of the very large database endowment , the google researchers said webpages would be allocated trustworthiness scores
they said : ' quality assessment for web sources is of tremendous importance in web search
' it has been traditionally evaluated using exogenous signals such as hyperlinks and browsing history
' for example , the gossip websites listed ( see image below ) mostly have high pagerank scores , but would not generally be considered reliable
' conversly , some less popular websites nevertheless have very accurate information
' ' we address the fundamental question of estimating how trustworthy a given web source is
' informally , we define the trustworthiness or accuracy of a web source as the probability that it contains the correct value for a fact ( such as barack obama 's nationality ) , assuming that it mentions any value for that fact
' currently web searches are ranked by , among other things , the number of incoming links to a page to help google 's search bots determine the quality of the link
this , however , is really only a measure of the popularity of a webpage rather than the accuracy of the information it contains
google researchers have developed a new algorithm that looks for facts and inaccuracies on web pages webpages containing inaccurate information can be widely shared and linked to by blogs and other external sites , causing them to feature high up in google search results
however , xin luna dong , wei zhang and colleagues at google have designed a way of automatically extracting information from webpages and ranking them for trustworthiness
they use a system they have called knowledge-based trust , which pulls facts from many pages and then jointly estimates the correctness and accuracy of these
it then counts the number of incorrect facts on a page to give it a trust score
to help the software the team has developed draws on google 's fast knowledge vault - a store of facts that have been pulled off the internet and are unanimously agreed on as being true
the software extracts what are known as ' knowledge triplets ' - such as barack obama , nationality , us
the list of the top five gossip sites above currently rank in the top 15 per cent of search results but google researchers say that using their new algorithm these will be religated to the bottom half of search results websites that contain contradictory information are then moved down the rankings on internet searches
in a test of their system , the researchers applied their model to 2.8 billion pages on the internet and were able to reliably predict the trustworthiness of 119 million pages and 5.6 million websites
giving an example of the nationality of us president barack obama , they say their system can distinguish between sources that are correct , those that are wrong and those read incorrectly by information extraction software
when the team applied their algorithm to a list of top 15 gossip websites including yahoo! omg ! , tmz , e online , and gawker they found that they ranked in the bottom 50 per cent of searches
the researchers said : ' in other words , they are considered less trustworthy than half of the websites
' however , under the current search system used by google , they rank among the top 15 per cent
in another example the paper describes how forums are also another source of poor information
they said : ' for instance , we discovered that answers.yahoo.com says that ' catherine zeta-jones is from new zealand ' , although she was born in wales according to wikipedia
' a spokesman for google said there were no specific plans to use the algorithm in their public search engine yet
he said : ' this was research - we do nât have any specific plans to implement it in our products
we publish hundreds of research papers every year